{"cells":[{"cell_type":"markdown","source":["## Decision Trees"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"code","source":["import sklearn.datasets as datasets\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom IPython.display import Image \n\nfrom sklearn.utils import resample\n"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## The digit classification dataset"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["D=datasets.load_digits()\nprint(D.keys())"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;])\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["print(D['DESCR'])"],"metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">.. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 5620\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Lets look at one example"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["k=10\nprint('label: ',D['target'][k])\nprint('image:\\n',D['images'][k])"],"metadata":{"hide_input":false,"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">label:  0\nimage:\n [[ 0.  0.  1.  9. 15. 11.  0.  0.]\n [ 0.  0. 11. 16.  8. 14.  6.  0.]\n [ 0.  2. 16. 10.  0.  9.  9.  0.]\n [ 0.  1. 16.  4.  0.  8.  8.  0.]\n [ 0.  4. 16.  4.  0.  8.  8.  0.]\n [ 0.  1. 16.  5.  1. 11.  3.  0.]\n [ 0.  0. 12. 12. 10. 10.  0.  0.]\n [ 0.  0.  1. 10. 13.  3.  0.  0.]]\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["print(D['data'][k])"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[ 0.  0.  1.  9. 15. 11.  0.  0.  0.  0. 11. 16.  8. 14.  6.  0.  0.  2.\n 16. 10.  0.  9.  9.  0.  0.  1. 16.  4.  0.  8.  8.  0.  0.  4. 16.  4.\n  0.  8.  8.  0.  0.  1. 16.  5.  1. 11.  3.  0.  0.  0. 12. 12. 10. 10.\n  0.  0.  0.  0.  1. 10. 13.  3.  0.  0.]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["### Decision tree learning algorithm\nSteps:"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["1) Define decision tree object\n```python\ndtree=DecisionTreeClassifier(max_depth=4)\n```"],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["2) Train Tree (action taken on object, no assignment needed)\n```python\ndtree.fit(D.data,D.target)\n```"],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["3) Use tree to make predictions\n```python\ndtree.predict(D.data)\n```"],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["To learn more about learning decision trees in `sklearn` see [Reference](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"],"metadata":{"slideshow":{"slide_type":"skip"}}},{"cell_type":"markdown","source":["## Visualization of a Decision Tree"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["### Tree Depth=1\n\nNote that with a tree of depth on you can only partition the labels into two sets."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["dtree=DecisionTreeClassifier(max_depth=1)\ndtree.fit(D.data,D.target)"],"metadata":{"hide_input":true,"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=1,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter=&#39;best&#39;)</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### Second step (Depth=2)"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["dtree=DecisionTreeClassifier(max_depth=2)\ndtree.fit(D.data,D.target)"],"metadata":{"hide_input":true,"scrolled":true,"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter=&#39;best&#39;)</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["### Third step (Depth=3)"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"code","source":["dtree=DecisionTreeClassifier(max_depth=3)\ndtree.fit(D.data,D.target)"],"metadata":{"hide_input":true,"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=3,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter=&#39;best&#39;)</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["## Over-Fitting"],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"markdown","source":["### Split the data into training and testing"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n#D_train, D_test, t_train, t_test = train_test_split(D.data, D.target, test_size=0.05,train_size=0.1, random_state=42)\nD_train, D_test, t_train, t_test = train_test_split(D.data, D.target, test_size=0.5, random_state=42)"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["def err(dtree,X,y):\n    yp=dtree.predict(X)\n    errs=np.sum(yp!=y)+0.0\n    return(errs/len(X)) "],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["train_err=[]\ntest_err=[]\nnode_count=[]\nfor i in range(1,30):\n    dtree=DecisionTreeClassifier(max_depth=i)\n    dtree.fit(D_train,t_train)\n    train_err.append(err(dtree,D_train,t_train)),\n    test_err.append(err(dtree,D_test,t_test))\n    node_count.append(dtree.tree_.node_count)"],"metadata":{"code_folding":[],"slideshow":{"slide_type":"skip"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["## Cross Validation\nWhen the data is small, we can't afford to split the data   \nto 50% train and 50% test. \n\nWe need to use a larger fraction of the data for training. Say 90% training.\n\nHowever, with 10% test we get a very crude estimate of the test error.\n\n**Cross Validation** is a method that allows 90% of the data to be used for training and still use all of the data for testing."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["For 90% we use **10-fold** cross validation. We split the data to 10 parts of approimately equal size. We then perform the following training and testing:\n\n1) Train on all but $1$ test on $1$  ($1$ is held out)  \n2) Train on all but $2$ test on $2$  ($2$ is held out)  \n3) ...    \n...  \n10) Train on all but $10$ test on $10$ ($10$ is held out)"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["Note that each example in the datasets appears exactly once in the test set. By summing the test errors on all of the held out parts we get a good estimate of the test error."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\nscores=[]\nfor i in range(1,30):  #Consider trees of depth 1-30\n    dtree=DecisionTreeClassifier(max_depth=i)\n    scores.append(np.mean(cross_val_score(dtree, D.data, D.target, cv=10)))"],"metadata":{"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["In this case we see that cross validation is not necessary because our data is large enough."],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"markdown","source":["## Confusion matrix\nWhen we have more than two possible labels, there are different types of mistakes. \n\nTo gain some insight into the types of mistakes, we use the **Confusion Matrix**"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\ndtree=DecisionTreeClassifier(max_depth=8)\ndtree.fit(D_train,t_train)\nA=confusion_matrix(t_test,dtree.predict(D_test))\n\nprint('pred label  ',''.join(['%3d'%j for j in range(10)]))\nfor i in range(A.shape[0]):\n    print('true label %d'%i,A[i,:])"],"metadata":{"hide_input":true,"slideshow":{"slide_type":"subslide"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">pred label     0  1  2  3  4  5  6  7  8  9\ntrue label 0 [79  2  0  0  0  1  0  0  0  0]\ntrue label 1 [ 2 65  4  0  4  9  2  0  3  0]\ntrue label 2 [ 0  1 75  3  0  0  0  0  4  0]\ntrue label 3 [ 0  2  6 69  0  4  0  4  3  5]\ntrue label 4 [ 0  4  0  0 83  2  1  1  2  0]\ntrue label 5 [ 0  3  1  7  2 80  1  0  3  2]\ntrue label 6 [ 0  8  1  0  4  2 82  0  1  0]\ntrue label 7 [ 0  1  2  0  0  2  0 79  0  3]\ntrue label 8 [ 0  9  2  6  0  7  4  1 53  1]\ntrue label 9 [ 0  0  1  3  2  6  0  2  2 76]\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["* `8,5,3,1' are confused with each other"],"metadata":{"slideshow":{"slide_type":"fragment"}}},{"cell_type":"code","source":["choice=[1,3,5,8]\nA[choice,:][:,choice]"],"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[21]: array([[65,  0,  9,  3],\n       [ 2, 69,  4,  3],\n       [ 3,  7, 80,  3],\n       [ 9,  6,  7, 53]])</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["## Summary\n* Decision trees greedily imporve the purity function\n* Decision trees overfit if their depth is too large\n* Cross validation is more accurate that a single train/test split when the training data is small. \n* Confusion matrix identify label pairs that are often confused with each other."],"metadata":{"slideshow":{"slide_type":"subslide"}}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"Decision_Trees","notebookId":1923096657762714,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"toc":{"title_sidebar":"Contents","nav_menu":{},"sideBar":true,"number_sections":true,"skip_h1_title":false,"base_numbering":1,"toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false,"title_cell":"Table of Contents"},"celltoolbar":"Slideshow","varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
